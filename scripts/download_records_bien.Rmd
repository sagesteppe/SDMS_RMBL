---
title: "Download records from bien"
author: "steppe"
date: "3/31/2022"
output: pdf_document
---

```{r load libraries, results='hide', message=F, warning=F}
library(tidyverse) # data tidying
library(sf) # spatial data compliant with tidyverse
library(here)
library(BIEN)
set.seed(12)

source(here::here('scripts/sdm_functions.R'))
```

```{r import coords and prepare for upload to database}
poly_coords <- st_read(
  paste0(here(), '/data/processed/BIEN_polygons', '2022-03-31', '.shp'), 
  quiet = T) 

poly100 <- poly_coords[7,]
poly100_bb <- poly100 %>%  
  st_bbox()

rm(poly_coords)
```


# We will download all species occurrence records from within a 100 km radius of the study area. 

```{r download records from BIEN, eval = F}
poly_100_recs <- BIEN_occurrence_box(min.lat  = poly100_bb[2],
                                     max.lat  = poly100_bb[4],
                                     min.long = poly100_bb[1],
                                     max.long = poly100_bb[3]
                                     )

write.csv(poly_100_recs, paste0(here(), '/data/raw/100km_bien_occurrence.csv'))
rm(poly100_bb)
```


```{r Create grids which repeat regularly to extract land ownership too}
grids <- st_make_grid(st_transform(poly100, 32613), 
             cellsize = 5000,
             square = F, 
             flat_topped = T) %>% 
  st_intersection(., st_transform(poly100, 32613))

pad <- st_read(paste0(here(), 
         '/data/raw/PADUS2_1_Region7_Shapefile/PADUS2_1Fee_Region7.shp'),
  quiet = T) %>% 
  filter(State_Nm == 'CO') %>% 
  dplyr::select(Category, Mang_Name, d_Des_Tp)

grids <- st_transform(grids, st_crs(pad)) %>% 
  st_as_sf() %>% 
  mutate(ID = 1:nrow(.))

pad_grids <- st_intersection(pad, grids)

pad_grids1 <- st_as_sf(pad_grids) %>% 
 # group_by(Category, Mang_Name, d_Des_Tp, ID) %>% 
  #summarize(geometry = st_union(geometry)) %>% 
  mutate(PL_Area = as.numeric(st_area(.)))

grids <- grids %>% 
  mutate(GR_Area = as.numeric(st_area(.)))

public_lands_ownership <- left_join(grids,
                  st_drop_geometry(pad_grids1),
                  by = 'ID') %>% 
  group_by(ID, Category, Mang_Name, d_Des_Tp) %>% 
  mutate(PL_Area = replace_na(PL_Area, 0)) %>% 
  mutate(PL_Area = sum(PL_Area)) %>% 
  distinct(ID, .keep_all = T) %>% 
  ungroup() %>% 
  group_by(ID) %>% 
  mutate('PropPublicLand' = sum(PL_Area)/GR_Area) %>% 
  group_by(ID, Category, Mang_Name, d_Des_Tp) %>% 
  mutate('PropOfGrid' = PL_Area/GR_Area)

rm(pad_grids, pad_grids1, pad)
```


```{r load in and process tabular data}
poly_100_recs <- read.csv(paste0(here(), '/data/raw/100km_bien_occurrence.csv'))
poly_100_recs <- poly_100_recs %>%
  st_as_sf(coords = c(x = 'longitude', y = 'latitude'), crs = 4326) %>% 
  dplyr::select('scrubbed_species_binomial') 

poly100_t <- st_intersection(poly100, poly_100_recs)
poly100_t <- poly100_t[,3:4]

center <- st_centroid(poly100)
distances <- st_distance(center, poly100_t)
distances <- as.numeric(distances)
distances <- tibble(distances)

poly100_t_dist <- cbind(distances, poly100_t)
poly100_t_dist <- st_as_sf(poly100_t_dist) %>%
  distinct(scrubbed_species_binomial, distances, .keep_all = T)

poly100_unique <- poly100_t_dist %>% 
  group_by(scrubbed_species_binomial) %>%
  slice_min(n = 5, order_by = distances, with_ties = F) 

rm(center, poly_100_recs, distances, poly100_bb)
```


```{r Modelling Relationship between Proportion of Public Lands and Number of Records, warning = F}
poly100_t <- st_transform(poly100_t, st_crs(grids))
poly100_t <- poly100_t %>% 
  distinct(scrubbed_species_binomial, geometry, .keep_all = T)
rcs_pr_grid <- st_intersection(grids, poly100_t) %>% 
  count(ID) %>% 
  st_drop_geometry()

public_lands_ownership <- left_join(public_lands_ownership, rcs_pr_grid, by = 'ID') %>% 
  mutate(n = replace_na(n, 0))

ggplot() +
  geom_sf(data = poly100) +
  geom_sf(data = public_lands_ownership, aes(fill = n)) +
  theme_void()

rm(rcs_pr_grid)
```

# DETERMINE INFLATION OF RMBL RECORDS TO OTHER AREAS - DOWNSAMPLE. 

maybe just take the mean # of records from the adjacent cells? will then need to repeatedly subset the spp. richness from there...

The grid cell containing the RMBL records has a surplus of records relative to cells in other areas. It is challenging to model how many records may occur in this area without the Field Station Existence. Here we take an absurdly conservative approach to calculate the number of records which may exist in the area by taking the mean of the surrounding grid cells. The reason this approach will be conservative is that nearly all of the adjacent cells do **not** have road access to them, or they are in fact largely occupied by a ton and private property. 

```{r Identify Cells Adjacent to RMBL to determine how large of sample to take from RMBL Cell, warning = F}
grids_out <- poly100 %>% 
  st_centroid() %>%
  st_transform(32613) %>% 
  st_buffer(5000) %>% 
  st_transform(., st_crs(grids)) %>%
  st_intersection(. , grids)

poly100 <- st_transform(poly100, st_crs(grids_out))
grid2rm <- st_intersection(st_centroid(poly100), grids_out) %>% 
  dplyr::select(ID)

grids_out <- grids_out %>% 
  filter(ID != grid2rm$ID)

recs2sample <- filter(public_lands_ownership, ID %in% grids_out$ID) %>% 
  #dplyr::select(ID, n, PropPublicLand) %>% 
  st_drop_geometry() %>% 
  distinct(ID, .keep_all = T) %>% 
  ungroup() %>% 
  summarize(third_quart = round(quantile(n, prob = 0.75), 0))

```

90! Records to subset per iteration, I would estimate the real value to be around 200-250 given the ability to just drive up to the area. Anyways. We went with the third quartile rather than the mean value due to the adjacent cells being in roadless wilderness areas, or for one having  a high proportion of private owernship. 


```{r prep data for going into sampler, warning = F}

poly100_t_dist <- poly100_t_dist %>% 
  mutate(Record_ID = 1:nrow(.))

target_grid <- filter(grids, ID == grid2rm$ID)

target_grid <- st_transform(target_grid, st_crs(poly100_t_dist))
focal_records <- st_intersection(target_grid, poly100_t_dist)

poly100_t_dist_sub <- poly100_t_dist %>% 
  filter(! Record_ID %in% focal_records$Record_ID) %>% 
  st_drop_geometry()
```


```{r subset the BIEN records and sample a proportion for logistic regression}

focal_records  <- focal_records %>% 
  dplyr::select(scrubbed_species_binomial, distances, Record_ID) %>% 
  st_drop_geometry()

abc <- spp_pres_sampler(target = focal_records, range = poly100_t_dist_sub, 
                        tSS = 87, rSS = 0.9, replicates = 200)

rm(focal_records, grid2rm, grids_out, poly100, poly100_t, public_lands_ownership, recs2sample, target_grid, poly100_t_dist_sub)
```


THESE RESULTS VERY CLOSE A COUPLE THINGS TO MAKE SURE TAXONOMY 1:1 MATCH UP - NOTE ORCHIDS ARE RESTRICTED FROM DB. 
```{r}
rmbl_plant_list <- read.csv(paste0(here(), '/data/processed/gothic_plant_list.csv'))[,c(1,4)]
rmbl_spp <- rmbl_plant_list$binomial
  
testing_data <- left_join(abc, poly100_t_dist, by = 'Record_ID') %>% 
  dplyr::select(-geometry) %>% 
  group_by(Iteration, scrubbed_species_binomial) %>% # the record further in space is
  slice_min(n = 1, order_by = distances) %>%  # pruned in this step 
  left_join(., rmbl_plant_list, by = c('scrubbed_species_binomial' ='binomial')) %>% 
  mutate(occurrence = if_else(!is.na(family), 1, 0) ) 

all_spp <- poly100_unique %>%  
  distinct(scrubbed_species_binomial, .keep_all = T)

```


```{r check for synonymy, message = F, results = 'hide', warning = F}
library(WorldFlora)
#WFO.download()
WFO.remember()

rmbl_spp <- sort(rmbl_spp)
spec.test <- data.frame(spec.name=c(rmbl_spp))
target_results <- WFO.synonyms(spec.data=spec.test, WFO.data=WFO.data, counter=1, verbose=F)

spec.test1 <- poly100_unique %>% 
  distinct(scrubbed_species_binomial) %>% 
  pull()
museum_results <- WFO.synonyms(spec.data=spec.test1, WFO.data=WFO.data, counter=1, verbose=TRUE)

length(rmbl_spp) <- nrow(target_results)
length(spec.test1) <- nrow(museum_results)

rmbl_spp <- cbind('Species' = rmbl_spp, target_results)
target_spp <- cbind('Species' = spec.test1, museum_results)

rm(spec.test, spec.test1, museum_results, WFO.data)
```


```{r clean species lists}

# i decided the results were good enough. 

rmbl_spp_CLEAN <- rmbl_spp %>% 
  filter(Species == scientificName)
rmbl_spp_DURTY <- rmbl_spp %>% 
  filter(Species != scientificName)

target_spp_CLEAN <- target_spp %>% 
  filter(Species == scientificName)
target_spp_DURTY <- target_spp %>% 
  filter(Species != scientificName)


# Here we will remove the species which were matched to genus but not to species
target_spp_CLEAN <- target_spp_DURTY %>% 
  mutate(scientificName = str_replace_all(scientificName, " ", "_")) %>% 
  filter(!str_detect(scientificName, '_')) %>% # WE WILL KEEP THE INPUT SPECIES HERE...
  mutate(scientificName = str_replace_all(scientificName, "_", " ")) %>% 
  mutate(scientificName = Species) %>% 
  bind_rows(., target_spp_CLEAN)
rmbl_spp_CLEAN <- rmbl_spp_DURTY %>% 
  mutate(scientificName = str_replace_all(scientificName, " ", "_")) %>% 
  filter(!str_detect(scientificName, '_')) %>% 
  mutate(scientificName = str_replace_all(scientificName, "_", " ")) %>% 
  mutate(scientificName = Species) %>% 
  bind_rows(., rmbl_spp_CLEAN)

# now perform the opposite
rmbl_spp_DURTY <- rmbl_spp_DURTY %>% 
  mutate(scientificName = str_replace_all(scientificName, " ", "_")) %>% 
  filter(str_detect(scientificName, '_')) %>% 
  mutate(scientificName = str_replace_all(scientificName, "_", " "))
target_spp_DURTY <- target_spp_DURTY %>% 
  mutate(scientificName = str_replace_all(scientificName, " ", "_")) %>% 
  filter(str_detect(scientificName, '_')) %>% 
  mutate(scientificName = str_replace_all(scientificName, "_", " "))

# THESE ARE EPITHETS WHICH ARE VALID FOR BOTH LISTS
rmbl_spp_CLEAN <- rmbl_spp_DURTY %>% 
  filter(scientificName %in% target_spp_CLEAN$scientificName)  %>% 
  bind_rows(., rmbl_spp_CLEAN)
target_spp_CLEAN <- target_spp_DURTY %>% 
  filter(scientificName %in% rmbl_spp_CLEAN$scientificName) %>% 
  bind_rows(., target_spp_CLEAN)

rmbl_spp_DURTY <- rmbl_spp_DURTY %>% 
  filter(!scientificName %in% target_spp_CLEAN$scientificName) 
target_spp_DURTY <- target_spp_DURTY %>% 
  filter(!scientificName %in% rmbl_spp_CLEAN$scientificName) 

# AT THIS POINT I FIND ACCEPTING THE REST OF KEWS NOTATIONS ACCEPTABLE . I DO NOT AGREE
# WITH ALL (NOTABLY THE STIPEAE TRIBE RETAINED IN THE GENUS STIPA S.L.), BUT WILL ACCEPT THEM...

rmbl_spp_clean <- bind_rows(rmbl_spp_CLEAN, rmbl_spp_DURTY)
target_spp_clean <- bind_rows(target_spp_CLEAN, target_spp_DURTY)

rm(rmbl_spp_CLEAN, rmbl_spp_DURTY, target_spp_CLEAN, target_spp_DURTY, target_spp_CLEAN, rmbl_spp_CLEAN, target_spp_clean, rmbl_spp_clean)
```

```{r Append presence absence to each museum record species}

target_spp <- target_spp %>% 
  dplyr::select(scientificName, scientificNameAuthorship, Species) %>% 
  mutate(Occurrence = if_else(.$scientificName %in% rmbl_spp$scientificName, 1, 0)) 

testing_data <- left_join(testing_data, target_spp, by = c('scrubbed_species_binomial'='Species')) %>% 
  dplyr::select(-scrubbed_species_binomial)

rm(target_spp)
```


## Assess Propotion of True Records

```{r, eval = F, warning = F}
td1 <- testing_data %>% 
  arrange(distances) %>% 
  ungroup() %>% 
  mutate(roll_sum = RcppRoll::roll_sum(occurrence, 5000, align = "right", fill = NA)/5000)

ggplot(td1, aes(x = distances, y = roll_sum)) +
  geom_jitter(alpha = 0.01) +
  geom_smooth()

td1$cuts <- cut(td1$distances, breaks = c(seq(1:50)*2000))

ggplot(td1, aes(x = cuts, y = roll_sum)) +
  geom_density(stat = 'identity')

```


## Logistic Regression Approach

split data set
```{r split data set, echo = F}
testing_data$distances <- testing_data$distances * -1
index <- caret::createDataPartition(testing_data$Occurrence, p = .70, list = FALSE)
train <- testing_data[ index, ]
test  <- testing_data[-index, ]
rm(index)

plot(vs ~ hp, data=mtcars, col="steelblue")
lines(vs ~ hp, newdata, lwd=2)

```


```{r fit model, echo = F, message = F}
allspecies_log <- glm(Occurrence ~ distances, data = train, family = "binomial")
mod_sum <- broom::tidy(allspecies_log)
mod_sum$p.value <- '< 0.001'
confint_res <- confint(allspecies_log)
rownames(confint_res) <- NULL
model_sum_tab <- cbind(mod_sum[,1], confint_res[,1], mod_sum[,2], confint_res[,2], mod_sum[,3:5])
colnames(model_sum_tab)[2] <- c('CI 2.5')
colnames(model_sum_tab)[4] <- c('CI 97.5')
knitr::kable(model_sum_tab)
```

We can present results using a classification table, more importantly this table will be  used to calculate a variety of metrics for evaluating the ability of the model to accurately classify observations. 
```{r Classification Table, echo = F}
pred_prob <- predict(allspecies_log, test, type = "response")
# Create a probability table for the TRAINING Data
train$pred_class <- ifelse(allspecies_log$fitted.values >= 0.5, 1, 0)
ctab_train <- table(cut(train$distances, breaks = 2), cut(train$occurrence,
                                                      breaks = 2))
# now repeat the process for the TEST data
test$pred_class <- ifelse(pred_prob >= 0.5, 1, 0)
ctab_test <- table(cut(test$distances, breaks = 2), cut(test$occurrence, breaks = 2))
ctab_report <- cbind(as.data.frame.matrix(ctab_train), 
                     as.data.frame.matrix(ctab_test))
rownames(ctab_report) <- c('Absence','Presence')
colnames(ctab_report) <- c('Absence_train', 'Presence_train', 
                           'Absence_test', 'Presence_test')
knitr::kable(ctab_report)
rm(ctab_report)
```

```{r Model Diagnostics, echo = F, warning = F}
accuracy_train <- sum(diag(ctab_train))/sum(ctab_train)*100
accuracy_test <- sum(diag(ctab_test))/sum(ctab_test)*100
# Calculate the True Positive Rate
Recall <- (ctab_train[2, 2]/sum(ctab_train[2, ]))*100
# Calculate the  True Negative Rate}
TNR <- (ctab_train[1, 1]/sum(ctab_train[1, ]))*100
# Calculate the Precision of the model 
Precision <- (ctab_train[2, 2]/sum(ctab_train[, 2]))*100
# Calculate F-score
F_Score <- (2 * Precision * Recall / (Precision + Recall))/100
# Reciever operator Curve
roc <- pROC::roc(train$occurrence, allspecies_log$fitted.values)
# Concordance}

evaluation_table <- data.frame(
  'Metric' = c('Accuracy (Training)','Accuracy (Test)','Recall',
                          'True Neg. Rate','Precision', 'F-Score', 'AUC'),
           'Value' = c(accuracy_train, accuracy_test, Recall, TNR, 
                       Precision, F_Score, as.numeric(roc[["auc"]]))
  )
evaluation_table$Value <- round(evaluation_table$Value, 2)
knitr::kable(evaluation_table)
rm(accuracy_train, accuracy_test, concordance, Recall, TNR, Precision, F_Score, roc, pair, ctab_train, ctab_test, evaluation_table)
```

```{r create prediction plot, warning = F, echo = F}
newdata1 <- with(testing_data, data.frame(distances = mean(distances)))
newdata1$probability <- predict(allspecies_log, newdata = newdata1, type = "response")
newdata2 <- with(testing_data, 
                 data.frame(distances = rep(seq(from = 0.1, to = 1, length.out = 10000), 4),
                                          distances = mean(distances)))
newdata3 <- cbind(newdata2, predict(allspecies_log, newdata = newdata2, type = "link",
    se = TRUE))
newdata3 <- within(newdata3, {
    PredictedProb <- plogis(fit)
    LL <- plogis(fit - (1.96 * se.fit))
    UL <- plogis(fit + (1.96 * se.fit))
})

testing_data$PredictedProb <- ifelse(testing_data$occurrence == 1, 1, 0)

ggplot(newdata3, aes(x = distances, y = PredictedProb)) + 
#  geom_ribbon(aes(ymin = LL, ymax = UL), fill = 'red', alpha = 0.8) + 
  geom_line() +
  geom_jitter(data = testing_data, aes(x = distances , y= PredictedProb), 
             alpha = 0.015, shape = 19, 
             height = 0.05, width = 0) +
  labs(title="Binomial Regression with Prediction Intervals and Data",
       x ="Distance (m)", y = "Occurrence") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylim(0,1) +
  theme_classic()

rm(newdata1, newdata2, newdata3)
```

