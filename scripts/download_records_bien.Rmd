---
title: "Download records from bien"
author: "steppe"
date: "3/31/2022"
output: pdf_document
---

```{r load libraries, results='hide', message=F, warning=F}
library(tidyverse) # data tidying
library(sf) # spatial data compliant with tidyverse
library(here)
library(BIEN)
set.seed(12)

source(here::here('scripts/sdm_functions.R'))
```

```{r import coords and prepare for upload to database}
poly_coords <- st_read(
  paste0(here(), '/data/processed/BIEN_polygons', '2022-03-31', '.shp'), 
  quiet = T) 

poly100 <- poly_coords[7,]
poly100_bb <- poly100 %>%  
  st_bbox()

rm(poly_coords)
```


# We will download all species occurrence records from within a 100 km radius of the study area. 

```{r download records from BIEN, eval = F}
poly_100_recs <- BIEN_occurrence_box(min.lat  = poly100_bb[2],
                                     max.lat  = poly100_bb[4],
                                     min.long = poly100_bb[1],
                                     max.long = poly100_bb[3]
                                     )

write.csv(poly_100_recs, paste0(here(), '/data/raw/100km_bien_occurrence.csv'))
rm(poly100_bb)
```


```{r Create grids which repeat regularly to extract land ownership too}
grids <- st_make_grid(st_transform(poly100, 32613), 
             cellsize = 5000,
             square = F, 
             flat_topped = T) %>% 
  st_intersection(., st_transform(poly100, 32613))

pad <- st_read(paste0(here(), 
         '/data/raw/PADUS2_1_Region7_Shapefile/PADUS2_1Fee_Region7.shp'),
  quiet = T) %>% 
  filter(State_Nm == 'CO') %>% 
  dplyr::select(Category, Mang_Name, d_Des_Tp)

grids <- st_transform(grids, st_crs(pad)) %>% 
  st_as_sf() %>% 
  mutate(ID = 1:nrow(.))

pad_grids <- st_intersection(pad, grids)

pad_grids1 <- st_as_sf(pad_grids) %>% 
 # group_by(Category, Mang_Name, d_Des_Tp, ID) %>% 
  #summarize(geometry = st_union(geometry)) %>% 
  mutate(PL_Area = as.numeric(st_area(.)))

grids <- grids %>% 
  mutate(GR_Area = as.numeric(st_area(.)))

public_lands_ownership <- left_join(grids,
                  st_drop_geometry(pad_grids1),
                  by = 'ID') %>% 
  group_by(ID, Category, Mang_Name, d_Des_Tp) %>% 
  mutate(PL_Area = replace_na(PL_Area, 0)) %>% 
  mutate(PL_Area = sum(PL_Area)) %>% 
  distinct(ID, .keep_all = T) %>% 
  ungroup() %>% 
  group_by(ID) %>% 
  mutate('PropPublicLand' = sum(PL_Area)/GR_Area) %>% 
  group_by(ID, Category, Mang_Name, d_Des_Tp) %>% 
  mutate('PropOfGrid' = PL_Area/GR_Area)

rm(pad_grids, pad_grids1, pad)
```


```{r load in and process tabular data}
poly_100_recs <- read.csv(paste0(here(), '/data/raw/100km_bien_occurrence.csv'))
poly_100_recs <- poly_100_recs %>%
  st_as_sf(coords = c(x = 'longitude', y = 'latitude'), crs = 4326) %>% 
  dplyr::select('scrubbed_species_binomial') 

poly100_t <- st_intersection(poly100, poly_100_recs)
poly100_t <- poly100_t[,3:4]

center <- st_centroid(poly100)
distances <- st_distance(center, poly100_t)
distances <- as.numeric(distances)
distances <- tibble(distances)

poly100_t_dist <- cbind(distances, poly100_t)
poly100_t_dist <- st_as_sf(poly100_t_dist) %>%
  distinct(scrubbed_species_binomial, distances, .keep_all = T)

poly100_unique <- poly100_t_dist %>% 
  group_by(scrubbed_species_binomial) %>%
  slice_min(n = 5, order_by = distances, with_ties = F) 

rm(center, poly_100_recs, distances, poly100_bb)
```


```{r Modelling Relationship between Proportion of Public Lands and Number of Records, warning = F}
poly100_t <- st_transform(poly100_t, st_crs(grids))
poly100_t <- poly100_t %>% 
  distinct(scrubbed_species_binomial, geometry, .keep_all = T)
rcs_pr_grid <- st_intersection(grids, poly100_t) %>% 
  count(ID) %>% 
  st_drop_geometry()

public_lands_ownership <- left_join(public_lands_ownership, rcs_pr_grid, by = 'ID') %>% 
  mutate(n = replace_na(n, 0))

ggplot() +
  geom_sf(data = poly100) +
  geom_sf(data = public_lands_ownership, aes(fill = n)) +
  theme_void()

rm(rcs_pr_grid)
```

# DETERMINE INFLATION OF RMBL RECORDS TO OTHER AREAS - DOWNSAMPLE. 

maybe just take the mean # of records from the adjacent cells? will then need to repeatedly subset the spp. richness from there...

The grid cell containing the RMBL records has a surplus of records relative to cells in other areas. It is challenging to model how many records may occur in this area without the Field Station Existence. Here we take an absurdly conservative approach to calculate the number of records which may exist in the area by setting the number of records form the cell containing RMBL equal to the most collections from an adjacent cell. The reason this approach will be conservative is that nearly all of the adjacent cells do **not** have road access to them, or they are in fact largely occupied by a ton and private property. 

```{r Identify Cells Adjacent to RMBL to determine how large of sample to take from RMBL Cell, warning = F}
grids_out <- poly100 %>% 
  st_centroid() %>%
  st_transform(32613) %>% 
  st_buffer(5000) %>% 
  st_transform(., st_crs(grids)) %>%
  st_intersection(. , grids)

poly100 <- st_transform(poly100, st_crs(grids_out))
grid2rm <- st_intersection(st_centroid(poly100), grids_out) %>% 
  dplyr::select(ID)

grids_out <- grids_out %>% 
  filter(ID != grid2rm$ID)

recs2sample <- filter(public_lands_ownership, ID %in% grids_out$ID) %>% 
  #dplyr::select(ID, n, PropPublicLand) %>% 
  st_drop_geometry() %>% 
  distinct(ID, .keep_all = T) %>% 
  ungroup() %>% 
  summarize(third_quart = max(n))
  #summarize(third_quart = round(quantile(n, prob = 0.75), 0))

```

137! Records to subset per iteration, I would estimate the real value to be around 200-250 given the ability to just drive up to the area; this is much in line with similar sub-alpine access via a good road. Anyways. We went with the third quantile rather than the mean value due to the adjacent cells being in road less wilderness areas, or for one having  a high proportion of private ownership. 

```{r prep data for going into sampler, warning = F}

poly100_t_dist <- poly100_t_dist %>% 
  mutate(Record_ID = 1:nrow(.))

target_grid <- filter(grids, ID == grid2rm$ID)

target_grid <- st_transform(target_grid, st_crs(poly100_t_dist))
focal_records <- st_intersection(target_grid, poly100_t_dist)

poly100_t_dist_sub <- poly100_t_dist %>% 
  filter(! Record_ID %in% focal_records$Record_ID) %>% 
  st_drop_geometry()
```


```{r subset the BIEN records and sample a proportion for logistic regression}

focal_records  <- focal_records %>% 
  dplyr::select(scrubbed_species_binomial, distances, Record_ID) %>% 
  st_drop_geometry()

abc <- spp_pres_sampler(target = focal_records, range = poly100_t_dist_sub, 
                        tSS = 137, rSS = 0.9, replicates = 250)

rm(focal_records, grid2rm, grids_out, poly100, public_lands_ownership, recs2sample, target_grid, poly100_t_dist_sub)
```


THESE RESULTS VERY CLOSE A COUPLE THINGS TO MAKE SURE TAXONOMY 1:1 MATCH UP - NOTE ORCHIDS ARE RESTRICTED FROM DB. 
```{r}
rmbl_plant_list <- read.csv(paste0(here(), '/data/processed/gothic_plant_list.csv'))[,c(1,4)]
rmbl_spp <- rmbl_plant_list$binomial
```

```{r, eval = F}
testing_data <- left_join(abc, poly100_t_dist, by = 'Record_ID') %>% 
  dplyr::select(-geometry) %>% 
  group_by(Iteration, scrubbed_species_binomial) %>% # the record further in space is
  slice_min(n = 1, order_by = distances) %>%  # pruned in this step 
  left_join(., rmbl_plant_list, by = c('scrubbed_species_binomial' ='binomial')) %>% 
  mutate(occurrence = if_else(!is.na(family), 1, 0) ) 

#all_spp <- poly100_unique %>%  
#  distinct(scrubbed_species_binomial, .keep_all = T)
```


```{r check for synonymy, message = F, results = 'hide', warning = F, eval=F}
library(WorldFlora)
#WFO.download()
WFO.remember()
     
rmbl_spp <- sort(rmbl_spp)
spec.test <- data.frame(spec.name=c(rmbl_spp))
target_results <- WFO.synonyms(spec.data=spec.test, WFO.data=WFO.data, counter=1, verbose=F)

spec.test1 <- poly100_unique %>% 
  distinct(scrubbed_species_binomial) %>% 
  pull()
museum_results <- WFO.synonyms(spec.data=spec.test1, WFO.data=WFO.data, counter=1, verbose=TRUE)

length(rmbl_spp) <- nrow(target_results)
length(spec.test1) <- nrow(museum_results)

rmbl_spp <- cbind('Species' = rmbl_spp, target_results)
target_spp <- cbind('Species' = spec.test1, museum_results)

write.csv(rmbl_spp, paste0(here(), '/data/processed/rmbl_spp_synonyms.csv'), row.names = F)
write.csv(target_spp, paste0(here(), '/data/processed/target_spp_synonyms.csv'), row.names = F)

rm(spec.test, spec.test1, museum_results, WFO.data)
```


```{r clean species lists, eval = F}

rmbl_spp <- read.csv(paste0(here(), '/data/processed/rmbl_spp_synonyms.csv'))
target_spp <- read.csv(paste0(here(), '/data/processed/target_spp_synonyms.csv'))

# i decided the results were good enough. this code still runs but put out what we basically put in.
rmbl_spp_CLEAN <- rmbl_spp %>% 
  filter(Species == scientificName)
rmbl_spp_DURTY <- rmbl_spp %>% 
  filter(Species != scientificName)

target_spp_CLEAN <- target_spp %>% 
  filter(Species == scientificName)
target_spp_DURTY <- target_spp %>% 
  filter(Species != scientificName)


# Here we will remove the species which were matched to genus but not to species
target_spp_CLEAN <- target_spp_DURTY %>% 
  mutate(scientificName = str_replace_all(scientificName, " ", "_")) %>% 
  filter(!str_detect(scientificName, '_')) %>% # WE WILL KEEP THE INPUT SPECIES HERE...
  mutate(scientificName = str_replace_all(scientificName, "_", " ")) %>% 
  mutate(scientificName = Species) %>% 
  bind_rows(., target_spp_CLEAN)
rmbl_spp_CLEAN <- rmbl_spp_DURTY %>% 
  mutate(scientificName = str_replace_all(scientificName, " ", "_")) %>% 
  filter(!str_detect(scientificName, '_')) %>% 
  mutate(scientificName = str_replace_all(scientificName, "_", " ")) %>% 
  mutate(scientificName = Species) %>% 
  bind_rows(., rmbl_spp_CLEAN)

# now perform the opposite
rmbl_spp_DURTY <- rmbl_spp_DURTY %>% 
  mutate(scientificName = str_replace_all(scientificName, " ", "_")) %>% 
  filter(str_detect(scientificName, '_')) %>% 
  mutate(scientificName = str_replace_all(scientificName, "_", " "))
target_spp_DURTY <- target_spp_DURTY %>% 
  mutate(scientificName = str_replace_all(scientificName, " ", "_")) %>% 
  filter(str_detect(scientificName, '_')) %>% 
  mutate(scientificName = str_replace_all(scientificName, "_", " "))

# THESE ARE EPITHETS WHICH ARE VALID FOR BOTH LISTS
rmbl_spp_CLEAN <- rmbl_spp_DURTY %>% 
  filter(scientificName %in% target_spp_CLEAN$scientificName)  %>% 
  bind_rows(., rmbl_spp_CLEAN)
target_spp_CLEAN <- target_spp_DURTY %>% 
  filter(scientificName %in% rmbl_spp_CLEAN$scientificName) %>% 
  bind_rows(., target_spp_CLEAN)

rmbl_spp_DURTY <- rmbl_spp_DURTY %>% 
  filter(!scientificName %in% target_spp_CLEAN$scientificName) 
target_spp_DURTY <- target_spp_DURTY %>% 
  filter(!scientificName %in% rmbl_spp_CLEAN$scientificName) 

# AT THIS POINT I FIND ACCEPTING THE REST OF KEWS NOTATIONS ACCEPTABLE . I DO NOT AGREE
# WITH ALL (NOTABLY THE STIPEAE TRIBE RETAINED IN THE GENUS STIPA S.L.), BUT WILL ACCEPT THEM...

rmbl_spp_clean <- bind_rows(rmbl_spp_CLEAN, rmbl_spp_DURTY)
target_spp_clean <- bind_rows(target_spp_CLEAN, target_spp_DURTY)

rm(rmbl_spp_CLEAN, rmbl_spp_DURTY, target_spp_CLEAN, target_spp_DURTY, target_spp_clean, rmbl_spp_clean)
```


```{r, message = F, results = 'hide', eval = F}
library(WorldFlora) # maybe you need this fam.
WFO.remember()

rmbl_spp <- filter(rmbl_spp, !is.na(Species))
rmbl_fam <- wfo_family_finder(rmbl_spp)
target_fam <- wfo_family_finder(target_spp)

# the same two records from both sets were not returned correctly
library(data.table)
setDT(rmbl_fam) # gotta use that slick that stuff. 
rmbl_fam[scientificName == 'Adoxa moschatellina',
         `:=`(Group = 'angiosperms', Family = "Adoxaceae", Order = 'Dipsacales', 
             Node.1 = 'Eudicots', Node.2 = 'Superasterids', Node.3 = 'Asterids',
             Node.4 = 'Campanulids')][scientificName == 'Sambucus racemosa',
                                      `:=`(Group = 'angiosperms', Family = "Adoxaceae", 
                                           Order = 'Dipsacales', Node.1 = 'Eudicots', Node.2 = 'Superasterids',
                                           Node.3 = 'Asterids', Node.4 = 'Campanulids')]
rmbl_fam <- tibble(rmbl_fam)
setDT(target_fam)
target_fam[scientificName == 'Adoxa moschatellina',
         `:=`(Group = 'angiosperms', Family = "Adoxaceae", Order = 'Dipsacales', 
             Node.1 = 'Eudicots', Node.2 = 'Superasterids', Node.3 = 'Asterids',
             Node.4 = 'Campanulids')][scientificName == 'Sambucus racemosa',
                                      `:=`(Group = 'angiosperms', Family = "Adoxaceae", 
                                           Order = 'Dipsacales', Node.1 = 'Eudicots', Node.2 = 'Superasterids',
                                           Node.3 = 'Asterids', Node.4 = 'Campanulids')]
target_fam <- tibble(target_fam)


write.csv(rmbl_fam, paste0(here(), '/data/processed/rmbl_spp_synonyms.csv'), row.names = F)
write.csv(target_fam, paste0(here(), '/data/processed/target_spp_synonyms.csv'), row.names = F)

rm(rmbl_fam, target_fam, WFO.data)
```

While running these results several limitations of my digitization of the Vascular Plant list became evident. I reviewed the RMBL absence records up to 20 km from the center of the field station and updated records which were either missed entirely (the new csv being imported), or not updated appropriately via any taxonomic cleaning. 
```{r Update names in dataset to reflect current taoxnomic opinions}

rmbl_spp <- read.csv(paste0(here(), '/data/processed/rmbl_spp_synonyms.csv')) %>% 
  rbind(read.csv(paste0(here(), '/data/processed/missed_rmbl_species.csv'))) # my digitization of the 
# rmbl vascular plant checklist left something to be desired, I missed (at least) these taxa. I looked
# them u pby handn on WFO and transcribed the info

rmbl_spp <- rmbl_spp %>% 
  mutate(scientificName = case_when(
    scientificName == 'Trollius' ~ 'Trollius albiflorus', 
    scientificName == 'Salix arctica' ~ 'Salix petrophila', 
    scientificName == 'Castilleja sulphurea' ~ 'Castilleja septentrionalis', 
    scientificName == 'Drymocallis arguta' ~ 'Potentilla arguta', 
    scientificName == 'Erysimum asperum' ~ 'Erysimum capitatum',  
    scientificName == 'Heracleum sphondylium' ~ 'Heracleum maximum',  
    scientificName == 'Potentilla anserina' ~ 'Argentea anserina',  
    scientificName == 'Sabulina rubella' ~ 'Minuartia rubella',
    scientificName == 'Smelowskia calycina' ~ 'Smelowskia americana',
    TRUE ~ scientificName
  ))

target_spp <- read.csv(paste0(here(), '/data/processed/target_spp_synonyms.csv')) %>% 
  drop_na(Species)
```


```{r Append presence absence to each museum record species}
target_spp <- target_spp %>% 
  dplyr::select(Group:Node.1, Species, scientificName) %>% 
  mutate(occurrence = if_else(.$scientificName %in% rmbl_spp$scientificName, 1, 0)) %>% 
  na_if("") %>% 
  mutate(Node.1 = replace_na(Node.1, 'Vascular')) %>% 
  mutate('PrtyFlrs' = case_when(
    Group != 'angiosperms' ~ 0,
    Group == 'angiosperms' & Order == 'Poales' ~ 0,
    TRUE ~ 1
  ))

testing_data <- left_join(abc, poly100_t_dist, by = 'Record_ID') %>% 
 # dplyr::select(-geometry) %>% 
  group_by(Iteration, scrubbed_species_binomial) %>% # the record further in space is
  slice_min(n = 1, order_by = distances) %>%  # pruned in this step 
  left_join(., target_spp, by = c('scrubbed_species_binomial' ='Species')) # %>% 
#  mutate(occurrence = if_else(scrubbed_species_binomial %in% scientificName, 1, occurrence))

write.csv(rmbl_spp, paste0(here(), '/data/processed/rmbl_wfo_synonyms.csv'), row.names = F)

rm(target_spp, rmbl_spp, abc, poly100_unique, poly100_t)
rm(list = lsf.str())
```


## Assess Propotion of True Records

```{r}

absent_spp <- testing_data %>% 
  filter(occurrence == 0) %>% 
  arrange(distances) %>% 
  ungroup() %>% 
  distinct(scrubbed_species_binomial, .keep_all = T)

# I made it to 17km out
rm(absent_spp, grids, rmbl_plant_list)
```


```{r, eval = F, warning = F}

#td1 <- testing_data %>% 
#  arrange(distances) %>% 
#  ungroup() %>% 
#  mutate(roll_sum = RcppRoll::roll_sum(occurrence, 500, align = "right", fill = NA)/500)
#td1$cuts <- cut(td1$distances, breaks = c(seq(0:50)*2000))
#ggplot(td1, aes(x = distances, y = roll_sum)) +
#  geom_density(stat = 'identity')

```


## Logistic Regression Approach

I attempted to fit Poisson, Negative binomial, zero inflated negative binomial models, but they did not do so hot. Instead the much more useful Logistic regression is fit here. 

split data set
```{r split data set, echo = F}
index <- caret::createDataPartition(testing_data$occurrence, p = .70, list = FALSE)
train <- testing_data[ index, ]
test  <- testing_data[-index, ]
rm(index)
```


```{r fit model, echo = F, message = F}
allspecies_log <- glm(occurrence ~ distances, data = train, family = "binomial")
mod_sum <- broom::tidy(allspecies_log)
mod_sum$p.value <- '< 0.001'
confint_res <- confint(allspecies_log)
rownames(confint_res) <- NULL
model_sum_tab <- cbind(mod_sum[,1], confint_res[,1], mod_sum[,2], confint_res[,2], mod_sum[,3:5])
colnames(model_sum_tab)[2] <- c('CI 2.5')
colnames(model_sum_tab)[4] <- c('CI 97.5')
knitr::kable(model_sum_tab)
```

We can present results using a classification table, more importantly this table will be  used to calculate a variety of metrics for evaluating the ability of the model to accurately classify observations. 

```{r Classification Table, echo = F}
pred_prob <- predict(allspecies_log, test, type = "response")
# Create a probability table for the TRAINING Data
train$pred_class <- ifelse(allspecies_log$fitted.values >= 0.5, 1, 0)
ctab_train <- table(cut(train$distances, breaks = 2), cut(train$occurrence,
                                                      breaks = 2))
# now repeat the process for the TEST data
test$pred_class <- ifelse(pred_prob >= 0.5, 1, 0)
ctab_test <- table(cut(test$distances, breaks = 2), cut(test$occurrence, breaks = 2))
ctab_report <- cbind(as.data.frame.matrix(ctab_train), 
                     as.data.frame.matrix(ctab_test))
rownames(ctab_report) <- c('Absence','Presence')
colnames(ctab_report) <- c('Absence_train', 'Presence_train', 
                           'Absence_test', 'Presence_test')
knitr::kable(ctab_report)

rm(ctab_report, pred_prob, confint_res)
```

```{r Model Diagnostics, echo = F, warning = F}
accuracy_train <- sum(diag(ctab_train))/sum(ctab_train)*100
accuracy_test <- sum(diag(ctab_test))/sum(ctab_test)*100
# Calculate the True Positive Rate
Recall <- (ctab_train[2, 2]/sum(ctab_train[2, ]))*100
# Calculate the  True Negative Rate}
TNR <- (ctab_train[1, 1]/sum(ctab_train[1, ]))*100
# Calculate the Precision of the model 
Precision <- (ctab_train[2, 2]/sum(ctab_train[, 2]))*100
# Calculate F-score
F_Score <- (2 * Precision * Recall / (Precision + Recall))/100
# Reciever operator Curve
roc <- pROC::roc(train$occurrence, allspecies_log$fitted.values)
# Concordance}

evaluation_table <- data.frame(
  'Metric' = c('Accuracy (Training)','Accuracy (Test)','Recall',
                          'True Neg. Rate','Precision', 'F-Score', 'AUC'),
           'Value' = c(accuracy_train, accuracy_test, Recall, TNR, 
                       Precision, F_Score, as.numeric(roc[["auc"]]))
  )
evaluation_table$Value <- round(evaluation_table$Value, 2)
knitr::kable(evaluation_table)
rm(accuracy_train, accuracy_test, concordance, Recall, TNR, Precision, F_Score, roc, pair, ctab_train, ctab_test, evaluation_table, mod_sum, model_sum_tab)
```

```{r create prediction plot, warning = F, echo = F}
newdata1 <- with(testing_data, data.frame(distances = mean(distances)))
newdata1$probability <- predict(allspecies_log, newdata = newdata1, type = "response")
newdata2 <- with(testing_data, 
                 data.frame(distances = rep(seq(from = min(testing_data$distances), to = max(testing_data$distances), length.out = 10000), 4),
                                          distances = mean(distances)))
newdata3 <- cbind(newdata2, predict(allspecies_log, newdata = newdata2, type = "link",
    se = TRUE))
newdata3 <- within(newdata3, {
    PredictedProb <- plogis(fit)
    LL <- plogis(fit - (1.96 * se.fit))
    UL <- plogis(fit + (1.96 * se.fit))
})

testing_data$PredictedProb <- ifelse(testing_data$occurrence == 1, 1, 0)

coin_flip <- filter(newdata3, PredictedProb >= 0.5) %>% 
  arrange(PredictedProb) %>% 
  slice_head() %>% 
  pull(distances) 
slim <- filter(newdata3, PredictedProb >= 0.25) %>% 
  arrange(PredictedProb) %>% 
  slice_head() %>% 
  pull(distances)

ggplot(newdata3, aes(x = distances, y = PredictedProb)) + 
  geom_jitter(data = testing_data, aes(x = distances , y= PredictedProb), 
             alpha = 0.015, shape = 19, 
             height = 0.05, width = 0) +
  geom_ribbon(aes(ymin = LL, ymax = UL), fill = 'red', alpha = 0.8) + 
  geom_line(lwd = 2) +
  
  geom_segment(aes(x = 0, xend = coin_flip, y = 0.50, yend = 0.50), lty = 3) +
  geom_segment(aes(x = 0, xend = slim, y = 0.25, yend = 0.25), lty = 3) +
  geom_segment(aes(x = coin_flip, xend = coin_flip, y = 0, yend = 0.50), lty = 3) +
  geom_segment(aes(x = slim, xend = slim, y = 0, yend = 0.25), lty = 3) +
  
  theme_classic() +
  labs(title="Binomial Regression with Prediction Intervals and Original Data",
       x ="Distance (m)", y = "Occurrence") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylim(0,1) 

rm(newdata1, newdata2, allspecies_log, test, train, newdata3)
```

### Attempt to improve resolution of records via inclusion of level 4 ecoregions


```{r Import and Buffer Field Station, warning = F}

ecoregions <- sf::read_sf(here("data/raw/us_eco_l4/us_eco_l4_no_st.shp")) %>% 
  st_transform(5070)

poly_coords <- st_read(
  paste0(here(), '/data/processed/BIEN_polygons', '2022-03-31', '.shp'), 
  quiet = T) 
poly100 <- poly_coords[7,] %>% 
  st_transform(st_crs(ecoregions))
poly100_bb <- poly100 %>%  
  st_bbox()

ecoregions <- ecoregions %>% 
  st_crop(poly100_bb)
rmbl <- st_centroid(poly100) %>% 
  st_transform(32613) %>% 
  st_buffer(10000) %>% 
  st_transform(st_crs(ecoregions))

ggplot() +
  geom_sf(data = ecoregions, aes(fill = US_L4NAME)) +
  geom_sf(data = rmbl, fill = NA, color = 'black', lwd = 1, lty = 5)

rm(poly_coords, poly100)
```

```{r intersect field station to ecoregion}

adj_L4 <- st_intersection(rmbl, ecoregions) %>% 
  distinct(US_L4NAME, .keep_all = T) %>% 
  pull(US_L4CODE)

poly100_t_dist <- st_transform(poly100_t_dist, st_crs(ecoregions))
L4 <- st_intersection(poly100_t_dist, ecoregions) %>% 
  dplyr::select(Record_ID, US_L4CODE, US_L4NAME) %>% 
  st_drop_geometry()

rm(poly100_t_dist, ecoregions, poly100_bb, rmbl)
```

The records within the distances where the midpoint of the logistic regression is may readily be acquired. If one is so interested they may acquire further records by focusing on 
```{r Identify probability values and pull out from dataset}

closest_records <- testing_data %>% 
  group_by(scrubbed_species_binomial) %>% 
  arrange(distances) %>% 
  left_join(L4, by = 'Record_ID') %>% 
  mutate(TargetL4 = if_else(US_L4CODE %in% adj_L4, 'TARGET', 'NON-TARGET')) %>% 
  slice_head() %>% 
  filter(PrtyFlrs == 1)

kept_recs <- closest_records %>% 
  filter(distances <= coin_flip)

half2quarter <- closest_records %>% 
  filter(distances >= slim)

prDF <- data.frame(
  'Habitat' = c(
    'Target',
    'Non-Target'
  ),
  'Sample' = c(
  nrow(subset(half2quarter, TargetL4 == 'TARGET')), 
  nrow(subset(half2quarter, TargetL4 == 'NON-TARGET'))
  ),
  'Occurrence' = c(
  nrow(subset(half2quarter, TargetL4 == 'TARGET' & occurrence == 1)),
  nrow(subset(half2quarter, TargetL4 == 'NON-TARGET' & occurrence == 1))
  ),
  'Proportion' = c(
  nrow(subset(half2quarter, TargetL4 == 'TARGET' & occurrence == 1)) /
    nrow(subset(half2quarter, TargetL4 == 'TARGET')),
  nrow(subset(half2quarter, TargetL4 == 'NON-TARGET' & occurrence == 1)) /
    nrow(subset(half2quarter, TargetL4 == 'NON-TARGET')) 
  )
)

prop.test(x = # relationship significant for the tail of the distribution, 
            # but not enough records to warrant chasing them ~ 30/222
            # relationship not as expected from 0.5 - 0.25 probability - no 
            # justification to warrant these results
            c(
              prDF$Occurrence[1], 
              prDF$Occurrence[2]
              ), 
          n = 
            c(
              prDF$Sample[1], 
              prDF$Sample[2]
              ),
          alternative = "greater", conf.level = 0.95
          )
# may be worth mentioning in results that some people may find this useful

rm(prDF, half2quarter, closest_records, coin_flip, slim, L4, adj_L4, testing_data)
```

write out results to csv, these results will all have SDM's generated of them.

```{r}
kept_recs <- kept_recs %>% 
  dplyr::select(scrubbed_species_binomial, scientificName)

write.csv(kept_recs1, paste0(here(), '/data/processed/spp2model.csv'), row.names = F)
rm(kept_recs1)
```

