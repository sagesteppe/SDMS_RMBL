---
title: "Determine cutoff threadholds for classifying presences in ensembles"
author: "steppe"
date: "3/17/2022"
output: pdf_document
---

```{r load libraries, results='hide', message=F, warning=F}
library(tidyverse) # data tidying
library(sf) # spatial data compliant with tidyverse
library(raster) # raster data
library(here)
library(rgbif)

source(here::here('scripts/sdm_functions.R'))
set.seed(12)
```


```{r Create Raster Stack of Predictons}
predictions_stack <- paste0(here(), '/results/maps/',
list.files(paste0(here(), '/results/maps/'),
           pattern = "*.tif$" ))
predictions_stack <- raster::stack(predictions_stack)
```

We will use the ecoregion bound to query GBIF for more presence records for testing ensembles for prediction thresholds. We will also used the records which were removed from the intial BIEN data due to high levels of spatial auto-correlation. 
```{r Import Pruned Occurrence Records for Presences}
pseudo_abs <- st_read(paste0(here(), '/data/processed/Pseudo_absences_regression.shp'), 
                      quiet = T)[,c(1:4,21)]
recs_rm_spatial_auto <- pseudo_abs %>% filter(occurrence == 2)
true_recs <- pseudo_abs %>% filter(!is.na(PlotKey) | occurrence == 1 | occurrence == 2)
  
ecoregion_bound <- st_read(paste0(here(),'/data/processed/SoRo_ecoregion_bound.shp'), quiet = T)
```

We will also import the AIM dataset to feed in more true absences. 
```{r Import BLM data for more True Absence Records}
AIM <- st_read(paste0(here(),'/data/processed/AIM_Records_study_area.shp'), quiet = T) %>% 
  dplyr::select(-data)
```

We will scrape GBIF for some new records to test our preidcictions against. 
```{r check GBIF for new presence records, eval = F}
wkt_boundary <-  ecoregion_bound %>%  
  st_transform(4326) %>% 
  dplyr::select(ncol(.)) %>% 
  st_bbox()

species_vec <- true_recs %>% distinct(binomial) %>% pull(binomial) %>% 
  gsub("_", " ", .)

gbif_results_raw <-  lapply(species_vec, gbif_collector)
gbif_results_raw <- gbif_results_raw %>% 
  bind_rows() %>% 
  st_as_sf(coords = c(x = 'decimalLongitude', y = 'decimalLatitude'), 
           crs = 4326)

# st_write(gbif_results_raw, paste0(here(),'/data/raw/gbif_data.shp'))
rm(wkt_boundary, species_vec)
rm(list=lsf.str())
```

```{r determine which GBIF records are duplicates of BIEN records, eval = F}

gbif <- st_read(paste0(here(),'/data/raw/gbif_data.shp'), quiet = T)
gbif <- gbif %>% separate(scntfcN, c("genus", "epithet"), 
                          extra = "drop", remove = F) %>% 
  unite('binomial', genus:epithet, sep = "_") %>% 
  st_transform( 32613)
ecoregion_bound <- st_transform(ecoregion_bound, 32613)

gbif <- st_intersection(ecoregion_bound, gbif)

true_recs <- st_transform(true_recs, 32613) %>% st_buffer(400)

true_recs_list <- split(true_recs, ~binomial)
gbif_list <-split(gbif, ~binomial)

# ensure reciprocal matching between data sets
gbif_list_sub <- gbif_list[which(names(gbif_list) %in% names(true_recs_list))]
true_recs_list_sub <- true_recs_list[which(names(true_recs_list) %in% names(gbif_list_sub))]

# we can then calculate st-distance between the members of the two data sets. 
results <- mapply(st_intersects, gbif_list_sub, true_recs_list_sub)
results <- map(results, tibble) 
results <- data.table::rbindlist(results)
colnames(results) <- 'Intersects'

gbif <- bind_rows(gbif_list_sub) %>% 
  cbind(., results) %>% 
  mutate(Intersects = if_else(as.character(Intersects) == "integer(0)", F, T)) %>% 
  mutate(Intersects = as.logical(Intersects)) %>% 
  filter(Intersects == F)

st_write(gbif, paste0(here(),'/data/processed/gbif_data_clean.shp'), quiet = T)

rm(true_recs_list, gbif_list, results, gbif_list_sub, true_recs_list_sub, gbif)
```

```{r remove some datasets which are generated for the above chunk when eval is false on it, echo = F}
rm(true_recs)
```


```{r followup gbif cleaning to remove duplicates, eval = F}
gbif <- st_read( paste0(here(),'/data/processed/gbif_data_clean.shp'), quiet = T)

gbif <- gbif %>% filter(occrrnS == 'PRESENT') %>% 
  st_cast('POINT') %>% 
  filter(bssOfRc %in% c('PRESERVED_SPECIMEN','HUMAN_OBSERVATION','OCCURRENCE')) %>% 
  distinct(binomial, geometry, .keep_all = T)
  
st_write(gbif, paste0(here(),'/data/processed/gbif_data_clean.shp'), quiet = T, append = F)
```

```{r join new GBIF data with thinned data and draw new absences}
gbif <- st_read( paste0(here(),'/data/processed/gbif_data_clean.shp'), quiet = T) %>% 
  dplyr::select(binomial) %>% 
  mutate(PlotKey = "") %>% 
  st_transform(predictions_stack@crs@projargs)
gbif$PlotKey <- na_if(gbif$PlotKey, "")

presences <- recs_rm_spatial_auto %>% 
  dplyr::select(binomial, PlotKey) %>% 
  st_transform(predictions_stack@crs@projargs) %>% 
  rbind(., gbif)

rm(gbif, recs_rm_spatial_auto)
```

Process united data set to assess validity for binomial logistic regression
```{r clean dataset for logistic regression}
# we need at least 10 absence and presences to run a binomial logistic regression
presences <- presences %>% 
  add_count(binomial) %>% 
  filter(n >= 10) %>% 
  dplyr::select(-n)

# should i check for spatial autocorrelation here???? - technically yes but...
```

obtain BLM absences to generate our full test set for regression
```{r collect BLM absences}
true_abs <- pseudo_abs %>%
  filter(!is.na(PlotKey) & occurrence == 0) 

presences_L <- st_transform(presences, 4269) %>%  
  mutate(occurrence = 1) %>% 
  split(., ~binomial)
true_abs_L <-  st_transform(true_abs, 4269) %>% 
  dplyr::select(binomial, PlotKey) %>% 
  mutate(occurrence = 0) %>% 
  split(., ~ binomial)
presences_L <- presences_L[which(names(presences_L) %in% names(true_abs_L))]
true_abs_L <- true_abs_L[which(names(true_abs_L) %in% names(presences_L))]

testing_set <- mapply(BinLogReg_abs, x = presences_L, y = true_abs_L,
                          SIMPLIFY = FALSE)
testing_set <- bind_rows(testing_set) 

testing_set %>% group_by(binomial) %>% 
  st_drop_geometry() %>% 
  count(occurrence) %>% 
  print()
rownames(testing_set) <- NULL

rm(true_abs, presences_L, true_abs_L, pseudo_abs)
```


```{r Extract SDM predictions to independent Presence/Absence Records, eval = F}

stacks <- names(predictions_stack) 
stacks <- gsub("_glm.*", "", stacks)
species <- testing_set %>% distinct(binomial) %>% pull()

# all species with a prediction and all species 
# with enough records to evaluate thresholds
positions <-intersect(stacks,species) 

# subset the evaluation data
testing_set <-testing_set %>% 
  filter(binomial %in% positions) %>% 
  st_transform(predictions_stack@crs@projargs)
testing_set_L <- split(testing_set, ~binomial) 
# to subset the raster stack to the species records
to_sub <- match(positions, stacks)
predictions_stack <- predictions_stack[[to_sub]]

# we now run a one to one extraction. 
values_data <-  vector(mode = "list", length = length(testing_set_L))
names(values_data) <- positions
for (i in 1:length(testing_set_L)){
  values_data[[i]] <- raster::extract(predictions_stack[[i]], testing_set_L[[i]],
                                      df = T)
}

# names(values_data) <- positions
values_data <- values_data %>% 
  map(~ rename(., Value = 2)) %>%
  bind_rows(., .id = "binomial")

testing_data <- testing_set %>% 
  st_drop_geometry( ) %>% 
  dplyr::select(occurrence) %>% 
  cbind(., values_data) %>%  
  dplyr::select(-ID)

write.csv(testing_data, paste0(here() , '/data/processed/logistic_regression_theshold_data.csv'), row.names = F)

rm(stacks, species, positions, to_sub, i, testing_set_L)
```


```{r clean up environment from processed data}
rm(AIM, ecoregion_bound, predictions_stack, presences, values_data, testing_set)
```

# Run models

 import and classify data, some values are slightly below and very slightly above 0 and 1, reclassify them here. 
```{r, echo = F}
set.seed(23)
testing_data <- read.csv(paste0(here() , '/data/processed/logistic_regression_theshold_data.csv'))
testing_data <- drop_na(testing_data)
testing_data <- testing_data %>% 
  mutate(Value = if_else(Value < 0, 0, Value)) %>% 
  mutate(Value = if_else(Value > 1, 1, Value))
```

split data set
```{r split data set}
index <- caret::createDataPartition(testing_data$occurrence, p = .70, list = FALSE)
train <- testing_data[ index, ]
test  <- testing_data[-index, ]
rm(index)
```


```{r fit model}
allspecies_log <- glm(occurrence ~ Value, data = train, family = "binomial")
mod_sum <- broom::tidy(allspecies_log)
mod_sum$p.value <- '< 0.001'

confint_res <- confint(allspecies_log)
rownames(confint_res) <- NULL

model_sum_tab <- cbind(mod_sum[,1], confint_res[,1], mod_sum[,2], confint_res[,2], mod_sum[,3:5])
colnames(model_sum_tab)[2] <- c('CI 2.5')
colnames(model_sum_tab)[4] <- c('CI 97.5')

knitr::kable(model_sum_tab)
```

We can present results using a classification table, more importantly this table will be  used to calculate a variety of metrics for evaluating the ability of the model to accurately classify observations. 
```{r Classification Table. echo = F}
pred_prob <- predict(allspecies_log, test, type = "response")

# Create a probabiliy table for the TRAINING Data
train$pred_class <- ifelse(allspecies_log$fitted.values >= 0.5, 1, 0)
ctab_train <- table(cut(train$Value, breaks = 2), cut(train$occurrence,
                                                      breaks = 2))

# now repeat the process for the TEST data
test$pred_class <- ifelse(pred_prob >= 0.5, 1, 0)
ctab_test <- table(cut(test$Value, breaks = 2), cut(test$occurrence, breaks = 2))

ctab_report <- cbind(as.data.frame.matrix(ctab_train), 
                     as.data.frame.matrix(ctab_test))
rownames(ctab_report) <- c('Absence','Presence')
colnames(ctab_report) <- c('Absence_train', 'Presence_train', 
                           'Absence_test', 'Presence_test')
knitr::kable(ctab_report)

rm(ctab_report)
```

```{r Model Diagnostics}
accuracy_train <- sum(diag(ctab_train))/sum(ctab_train)*100
accuracy_test <- sum(diag(ctab_test))/sum(ctab_test)*100
# Calculate the True Positive Rate
Recall <- (ctab_train[2, 2]/sum(ctab_train[2, ]))*100
# Calculate the  True Negative Rate}
TNR <- (ctab_train[1, 1]/sum(ctab_train[1, ]))*100
# Calculate the Precision of the model 
Precision <- (ctab_train[2, 2]/sum(ctab_train[, 2]))*100
# Calculate F-score
F_Score <- (2 * Precision * Recall / (Precision + Recall))/100

# Reciever operator Curve
roc <- pROC::roc(train$occurrence, allspecies_log$fitted.values)

# Concordance}
concordance <- InformationValue::Concordance(allspecies_log$y,
                                             allspecies_log$fitted.values)
conc <- concordance[["Concordance"]]
disc <- concordance[["Discordance"]]
tied <- concordance[["Tied"]]
pair <- concordance[["Pairs"]]

evaluation_table <- data.frame(
  'Metric' = c('Accuracy (Training)','Accuracy (Test)','Recall',
                          'True Neg. Rate','Precision', 'F-Score', 'AUC', 
                          'Concordance', 'Discordance', 'Tied', 'Pairs'),
           'Value' = c(accuracy_train, accuracy_test, Recall, TNR, 
                       Precision, F_Score, as.numeric(roc[["auc"]]),
                       conc, disc, tied, pair)
  )

evaluation_table$Value <- format(round(as.numeric(evaluation_table$Value), 2), nsmall = 2)
knitr::kable(evaluation_table)

rm(accuracy_train, accuracy_test, concordance, Recall, TNR, Precision, F_Score, roc,conc, disc, tied, pair, ctab_train, ctab_test, evaluation_table)
```

```{r create prediction plot}

newdata1 <- with(testing_data, data.frame(Value = mean(Value)))
newdata1$probability <- predict(allspecies_log, newdata = newdata1, type = "response")
newdata2 <- with(testing_data, 
                 data.frame(Value = rep(seq(from = 0.1, to = 1, length.out = 100), 4),
                                          Value = mean(Value)))

newdata3 <- cbind(newdata2, predict(allspecies_log, newdata = newdata2, type = "link",
    se = TRUE))

newdata3 <- within(newdata3, {
    PredictedProb <- plogis(fit)
    LL <- plogis(fit - (1.96 * se.fit))
    UL <- plogis(fit + (1.96 * se.fit))
})

testing_data$PredictedProb <- ifelse(testing_data$occurrence == 1, 1, 0)

ggplot(newdata3, aes(x = Value, y = PredictedProb)) + 
  geom_ribbon(aes(ymin = LL,ymax = UL), fill = 'red', alpha = 0.8) + 
  geom_line(size = 1) +
  geom_jitter(data = testing_data, aes(x = Value , y= PredictedProb), 
             alpha = 0.025, shape = 19, 
             height = 0.05, width = 0) +
  labs(title="Binomial Regression with Prediction Intervals and Data",
       x ="SDM Probability of Occurrence", y = "Occurrence") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylim(0,1) +
  theme_classic()

rm(newdata1, newdata2, newdata3)
```


